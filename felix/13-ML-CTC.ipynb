{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial, reduce\n",
    "from collections import deque\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# labelBaseMap = {\n",
    "#     0: \"A\",\n",
    "#     1: \"C\",\n",
    "#     2: \"G\",\n",
    "#     3: \"T\"\n",
    "# }\n",
    "\n",
    "possible_filenames = [\"/mnt/nvme/taiyaki_aligned/mapped_umi16to9.hdf5\",\n",
    "                      \"/Users/felix/MsC/DNA/mapped_umi16to9.hdf5\"]\n",
    "\n",
    "RNN_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(dac):\n",
    "    dmin = min(dac)\n",
    "    dmax = max(dac)\n",
    "    return [(d-dmin)/(dmax-dmin) for d in dac]\n",
    "\n",
    "def ohe(v):\n",
    "    tr = [0,0,0,0]\n",
    "    tr[v] = 1\n",
    "    return tr\n",
    "\n",
    "def change_shape(dac):\n",
    "    return [[x] for x in dac]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in possible_filenames:\n",
    "    if not os.path.isfile(filename):\n",
    "        pass\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        readIDs = list(h5file['Reads'].keys())\n",
    "        print(f\"{len(readIDs)} reads, keys: {list(h5file['Reads'][readIDs[0]].keys())}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRead(readID, filename, train_validate_split=0.8, min_labels=5, one_hot_encode=False):\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X  = []\n",
    "    test_y  = []\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        DAC = list(normalise(h5file['Reads'][readID]['Dacs'][()]))\n",
    "        RTS = deque(list(h5file['Reads'][readID]['Ref_to_signal'][()]))\n",
    "        if ohe:\n",
    "            REF = deque(h5file['Reads'][readID]['Reference'][()])\n",
    "        else:\n",
    "            REF = deque(h5file['Reads'][readID]['Reference'][()])\n",
    "        \n",
    "    # just get the number, (1-tvs) so that it can be compared to how many items are left    \n",
    "    train_validate_split = round(len(REF)*(1-train_validate_split))\n",
    "    \n",
    "    # -5 so that the first WHILE iteration afterwards cancels it out\n",
    "    curdacs  = deque( [[x] for x in DAC[RTS[0]:RTS[0]+RNN_LEN-5]], RNN_LEN ) # deque keeping `RNN_LEN` DACs\n",
    "    curdacts = RTS[0]+RNN_LEN-5 # the current HEAD\n",
    "    labels  = deque([]) # to hold the label for the sequence\n",
    "    labelts = deque([]) # to hold the timestamps for the labels\n",
    "\n",
    "    while RTS[0] < curdacts: # add the first RNN_LEN worth of labels to initialise\n",
    "        labels.append(REF.popleft())\n",
    "        labelts.append(RTS.popleft())    \n",
    "    \n",
    "    \n",
    "    while curdacts+5 < RTS[-1]-RNN_LEN:\n",
    "        curdacs.extend([[x] for x in DAC[curdacts:curdacts+5]])\n",
    "        curdacts += 5\n",
    "        \n",
    "        # add labels if new ones appeared\n",
    "        while RTS[0] < curdacts:\n",
    "            labels.append(REF.popleft())\n",
    "            labelts.append(RTS.popleft())    \n",
    "        \n",
    "        # pop from labels if we passed them\n",
    "        # sometimes the strand gets stuck and the deques drop to 0\n",
    "        while len(labelts) > 0 and labelts[0] < curdacts - RNN_LEN:\n",
    "            labels.popleft()\n",
    "            labelts.popleft()\n",
    "\n",
    "        # only add if more than 5 labels\n",
    "        if len(labels) > min_labels:\n",
    "            # add to train if more than tvs remain\n",
    "            if len(RTS) > train_validate_split:\n",
    "                train_X.append(list(curdacs))\n",
    "                train_y.append(list(labels))\n",
    "            else:\n",
    "                test_X.append(list(curdacs))\n",
    "                test_y.append(list(labels))\n",
    "                \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "# pp = partial(processRead, filename=filename, one_hot_encode=True)\n",
    "# tr_X, tr_y, te_X, te_y = pp(readIDs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(16)\n",
    "results_prim = pool.map(partial(processRead, filename=filename, one_hot_encode=True), readIDs[:16])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Pool done\")\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X  = []\n",
    "test_y  = []\n",
    "for thread in results_prim:\n",
    "    train_X.extend(thread[0])\n",
    "    train_y.extend(np.array(thread[1]))\n",
    "    test_X.extend(thread[2])\n",
    "    test_y.extend(thread[3])\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "test_X  = np.array(test_X)\n",
    "test_y  = np.array(test_y)\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"train_X\", train_X)\n",
    "# np.save(\"train_y\", train_y)\n",
    "# np.save(\"test_X\", test_X)\n",
    "# np.save(\"test_y\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = np.load(\"train_X.npy\")\n",
    "# train_y = np.load(\"train_y.npy\", allow_pickle=True)\n",
    "# test_X  = np.load(\"test_X.npy\")\n",
    "# test_y  = np.load(\"test_y.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "# print(train_X.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_X.shape)\n",
    "# print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE COME DAT ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Lambda\n",
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, LSTM\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "import numpy as np\n",
    "\n",
    "# device_hasgpu = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "# if device_hasgpu:\n",
    "#     gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "#     sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return kb.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(name=\"input\", shape=(200,1), dtype=\"float32\")\n",
    "inner = Conv1D(32, 3,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\", \n",
    "          input_shape=(200,1),\n",
    "          name=\"conv1d_1\")(input_data)\n",
    "inner = MaxPooling1D(pool_size=5, name=\"maxpool_1\")(inner)\n",
    "lstm_1a = LSTM(32,return_sequences=True, name=\"lstm_1a\")(inner)\n",
    "lstm_1b = LSTM(32, return_sequences=True, go_backwards=True, name=\"lstm_1b\")(inner)\n",
    "lstm_1_merged = Add()([lstm_1a, lstm_1b])\n",
    "\n",
    "inner = Dense(5, name=\"dense_1\")(lstm_1_merged)\n",
    "\n",
    "y_pred = Activation(\"softmax\", name=\"softmax\")(inner)\n",
    "\n",
    "# Model(inputs=input_data, outputs=y_pred).summary()\n",
    "\n",
    "labels = Input(name='the_labels', shape=[5], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out, name=\"my_model\")\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'the_input': train_X,\n",
    "    'the_labels': train_y,\n",
    "    'input_length': np.array([[39-2]]*len(train_X)),\n",
    "    'label_length': np.array([[len(y)] for y in train_y])\n",
    "}\n",
    "\n",
    "outputs = {'ctc': np.zeros([len(train_X)])} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
