{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "import jkbc.constants as constants\n",
    "import jkbc.files.torch_files as f\n",
    "import jkbc.utils.preprocessing as prep\n",
    "import jkbc.utils.postprocessing as pop\n",
    "import jkbc.utils as utils\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "BASE_DIR = Path(\"../..\")\n",
    "PATH_DATA = 'data/feather-files'\n",
    "PROJECT = 'jk-basecalling-v2' \n",
    "TEAM=\"jkbc\"\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, alphabet: str, beam_size = 25, beam_threshold=0.1):\n",
    "    # Predict signals\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    for input, (target, _, _) in iter(data_loader):\n",
    "        pred = model(input.to('cuda')).detach().cpu()\n",
    "        decoded = pop.decode(pred, alphabet, beam_size=beam_size, threshold=beam_threshold)\n",
    "        \n",
    "        predictions.append(decoded)\n",
    "        labels.append(target)\n",
    "    \n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_decoded(x, y, alphabet_values):\n",
    "    references = {}\n",
    "    predictions= {}\n",
    "    for batch in range(len(x)):\n",
    "        for index in range(len(x[batch])):\n",
    "            key = f'#{batch}#{index}#'\n",
    "            references[key] = pop.convert_idx_to_base_sequence(y[batch][index], alphabet_values)\n",
    "            predictions[key] = x[batch][index]\n",
    "    return references, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model_id, data_name, labels, predictions):\n",
    "    import jkbc.files.fasta as fasta\n",
    "    reference_dict, prediction_dict = map_decoded(predictions, labels, list(constants.ALPHABET.values()))\n",
    "    \n",
    "    '''\n",
    "    # make dicts ready to be saved\n",
    "    ref_merged = fasta.merge(reference_dict)\n",
    "    pred_dict = fasta.merge(prediction_dict)\n",
    "    '''\n",
    "    # save fasta\n",
    "    fasta.save_dicts(prediction_dict, reference_dict, f'predictions/{model_id}-{data_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(run_path, root):\n",
    "    config_path = wandb.restore('config.yaml', run_path=run_path, replace=True, root=root)\n",
    "    with open(config_path.name, 'r') as config_file:\n",
    "        config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_window_size(data_set):\n",
    "    with open(f'{data_set}/config.json', 'r') as fp:\n",
    "        data_config = json.load(fp)\n",
    "        window_size    = int(data_config['maxw']) #maxw = max windowsize\n",
    "        '''    \n",
    "        dimensions_out = int(data_config['maxl']) # maxl = max label length\n",
    "        min_label_len  = int(data_config['minl']) # minl = min label length\n",
    "        stride         = int(data_config['s'])\n",
    "        '''\n",
    "    return window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, window_size, device, run_path, root):\n",
    "    import jkbc.model as m\n",
    "    import jkbc.model.factory as factory\n",
    "    import jkbc.utils.bonito.tune as bonito\n",
    "\n",
    "    # Model\n",
    "    model_params = utils.get_nested_dict(config, 'model_params')['value']\n",
    "    model_config = bonito.get_bonito_config(model_params, double_kernel_sizes=False)\n",
    "\n",
    "    model, _ = factory.bonito(window_size, device, model_config)\n",
    "    predicter = m.get_predicter(model, device, '')\n",
    "\n",
    "    weights = wandb.restore('bestmodel.pth', run_path=run_path, replace=True, root=root)\n",
    "    # fastai requires the name without .pth\n",
    "    model_weights = '.'.join(weights.name.split('.')[:-1])\n",
    "    predicter.load(model_weights)\n",
    "    \n",
    "    return predicter.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [23:22<00:00, 1402.59s/it]\n",
      "100%|██████████| 1/1 [22:39<00:00, 1359.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ids = [\n",
    "    '2j9fzbx4', #swept-durian-82 # bonito\n",
    "    '5916pnqr', #sleek-serenity-251 - stupid\n",
    "    \n",
    "    '2eiadj4y', #eternal-deluge-448\n",
    "    '1ywu3vo9', #breezy-cosmos-408\n",
    "    '2d84exku', #scarlet-sound-417\n",
    "    'j6f2sn3v', #vibrant-puddle-433\n",
    "    '1c2vr2my'  #playful-oath-434\n",
    "]\n",
    "\n",
    "ids = ['117mrxzu']\n",
    "data_set = [('all', BASE_DIR/PATH_DATA/'all-other'), ('Bacillus', BASE_DIR/PATH_DATA/'bacillus')]\n",
    "batch_size=64\n",
    "alphabet = ''.join(constants.ALPHABET.values())\n",
    "\n",
    "errors = []\n",
    "for name, path in data_set:\n",
    "    window_size = get_window_size(path)\n",
    "    data = f.load_training_data(path) \n",
    "    test_dl, _ = prep.convert_to_dataloaders(data, split=1, batch_size=batch_size, drop_last=True)\n",
    "    for id in tqdm(ids):\n",
    "        #try:\n",
    "        run_path = f\"{TEAM}/{PROJECT}/{id}\"\n",
    "        root=f'wandb/{id}'\n",
    "\n",
    "        config = get_config(run_path, root)\n",
    "        model = get_model(config, window_size, DEVICE, run_path, root)\n",
    "\n",
    "        predictions, labels = predict(model, test_dl, alphabet, \n",
    "                                      beam_size = 25, beam_threshold=0.1)\n",
    "\n",
    "        save(id, name, labels, predictions)\n",
    "        #except:\n",
    "        #    errors.append((id, name))\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  4 12:02:42 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.116.00   Driver Version: 418.116.00   CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    66W / 350W |  29439MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     35129      C   ...dk/kbargs15/.conda/envs/jkbc/bin/python 11853MiB |\n",
      "|    0     64521      C   ...dk/kbargs15/.conda/envs/jkbc/bin/python 17569MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jkbc",
   "language": "python",
   "name": "jkbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
