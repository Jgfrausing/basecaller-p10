{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics + Reshape meaning\n",
    "Based on https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS is batch size; D_in is input dimension;\n",
    "# D_h is hidden dimension; D_out is output dimension.\n",
    "BS, D_in, D_h, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "It can quickly become hairy and difficult to compute the gradients correctly for a large network. Autograd in PyTorch alleviates this problem, by automatically calculating and keeping track of the gradients for your tensors. That is, if `requires_grad=True`. Essentially the gradients for tensor `x` will be another tensor `x.grad` calculated with respect to some scalar value (the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensors to hold input and outputs.\n",
    "# Default is requires_grad=False, which is what we want for these tensors, because\n",
    "# we don't need to update them using gradients.\n",
    "# Tensors are very much like numpy arrays, except that we can put them on the GPU!\n",
    "x = torch.randn(BS, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(BS, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights.\n",
    "# Now we want requires_grad=True so that we can compute the gradients when doing a backward pass.\n",
    "w1 = torch.randn(D_in, D_h, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 339.53570556640625\n",
      "199 1.9644684791564941\n",
      "299 0.016090137884020805\n",
      "399 0.0003336144145578146\n",
      "499 4.796755820279941e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: calculate y_pred using the weight tensors and ReLU (clamp)\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print the loss using operations on tensors.\n",
    "    # The loss is a tensor of shape (1,), i.e. a scalar\n",
    "    # loss.item gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Use autograd to calculate the backward pass. This will calculate the gradients\n",
    "    # in regard to the loss scalar for all tensors involved with requires_grad=True.\n",
    "    # And the results will be stored in the .grad field as a tensor\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent. We don't want to keep track of the operations\n",
    "    # using autograd, so we can wrap it in torch.no_grad() to avoid it.\n",
    "    # It is also possible to use tensor.data field, which shares the storage\n",
    "    # of the tensor, but without the tracking of operation history by autograd.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # The gradients accumulate automatically, so we need to reset them.\n",
    "        # Any torch functions ending in _ are in-place\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "While autograd is powerful, it is a bit too low level for most purposes. In PyTorch we use Modules, which are roughly equivalent to layers in a NN. Here is the same example as above, but using a nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BS, D_in)\n",
    "y = torch.randn(BS, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype # apparently float32 is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential is a Module that contains other Modules and applies them in sequence to produce its output.\n",
    "# nn.Linear applies a linear function and holds internal tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_h, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions for popular loss functions.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.059399127960205\n",
      "199 0.02680256962776184\n",
      "299 0.0006534525891765952\n",
      "399 2.0763705833815038e-05\n",
      "499 7.760471589790541e-07\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute y_pred using the model.\n",
    "    # Module objects override __call__, so you can call them like functions.\n",
    "    # nn.Module.__call__ :: Tensor -> Tensor\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Backward pass: compute gradients for all learnable parameters in the model \n",
    "    # i.e. values in the tensors with requires_grad=True\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent. We can access the weights using model.parameters(), which\n",
    "    # returns a list of tensors.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "    # Remember to zero the gradients!\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim\n",
    "Updating the weights manually is quite easy when we use a simple method such as stochastic gradient descent, but it becomes quite complex, when we want to use more sophisticated optimizers such as AdaGrad, RMSProp, Adam, Adam_ann, etc.\n",
    "PyTorch therefore provides an `optim` package, which abstracts the idea of an optimization algorithm and provides the commonly used algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BS, D_in)\n",
    "y = torch.randn(BS, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_h, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first parameter for most optimizer functions is the tensors, which it should update.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 77.20761108398438\n",
      "199 1.6605950593948364\n",
      "299 0.014822077006101608\n",
      "399 0.00010365447087679058\n",
      "499 5.915860015193175e-07\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # I am not sure why they call zero_grad first. Seems like they could have waited till the end.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()      \n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom nn Modules\n",
    "We can also create custom Modules and use them with the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_h, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_h)\n",
    "        self.linear2 = torch.nn.Linear(D_h, D_out)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        '''\n",
    "        forward takes a batch of x's and return y_pred. \n",
    "        It is what is called when you use the model as a function (model(x)).\n",
    "        '''\n",
    "        h_relu = self.linear1(xb).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BS, D_in)\n",
    "y = torch.randn(BS, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, D_h, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion is, AFAIK, the same as loss function in a ML context\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.6298742294311523\n",
      "199 0.07662727683782578\n",
      "299 0.0065430752001702785\n",
      "399 0.0008591677178628743\n",
      "499 0.00013513835438061506\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Shapes (NumPy / PyTorch)\n",
    "NumPy arrays and, AFAIK, also PyTorch tensors consist of two primary parts.\n",
    "- The **data buffer** which is just a block of raw elements,\n",
    "- and a **view** which describes how to interpret the data buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(12)\n",
    "a # the 'data buffer'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the data buffer:\n",
    "┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n",
    "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
    "└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape # this is a 'view'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(12,) means that we can index into the data buffer using a single index.\n",
    "\n",
    "i= 0    1    2    3    4    5    6    7    8    9   10   11\n",
    "┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n",
    "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
    "└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can also create a different view\n",
    "b = a.reshape((3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which *doesn't* alther the data buffer underneath, but it allows us to index in a different way.\n",
    "In this case using two indexes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i= 0    0    0    0    1    1    1    1    2    2    2    2\n",
    "j= 0    1    2    3    0    1    2    3    0    1    2    3\n",
    "┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n",
    "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
    "└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.reshape((1, 2, 1, 6, 1)) # Basically, any dimensions with length 1 are \"free\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i= 0    0    0    0    0    0    0    0    0    0    0    0\n",
    "j= 0    0    0    0    0    0    1    1    1    1    1    1\n",
    "k= 0    0    0    0    0    0    0    0    0    0    0    0\n",
    "l= 0    1    2    3    4    5    0    1    2    3    4    5\n",
    "m= 0    0    0    0    0    0    0    0    0    0    0    0\n",
    "┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n",
    "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
    "└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
