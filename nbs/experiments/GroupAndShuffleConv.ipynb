{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group and Shuffle Convolutions\n",
    "https://pytorch.org/hub/pytorch_vision_shufflenet_v2/\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, groups=1, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 5, 5]),\n",
       " tensor([[[[  0.,   1.,   2.,   3.,   4.],\n",
       "           [  5.,   6.,   7.,   8.,   9.],\n",
       "           [ 10.,  11.,  12.,  13.,  14.],\n",
       "           [ 15.,  16.,  17.,  18.,  19.],\n",
       "           [ 20.,  21.,  22.,  23.,  24.]],\n",
       " \n",
       "          [[ 25.,  26.,  27.,  28.,  29.],\n",
       "           [ 30.,  31.,  32.,  33.,  34.],\n",
       "           [ 35.,  36.,  37.,  38.,  39.],\n",
       "           [ 40.,  41.,  42.,  43.,  44.],\n",
       "           [ 45.,  46.,  47.,  48.,  49.]],\n",
       " \n",
       "          [[ 50.,  51.,  52.,  53.,  54.],\n",
       "           [ 55.,  56.,  57.,  58.,  59.],\n",
       "           [ 60.,  61.,  62.,  63.,  64.],\n",
       "           [ 65.,  66.,  67.,  68.,  69.],\n",
       "           [ 70.,  71.,  72.,  73.,  74.]],\n",
       " \n",
       "          [[ 75.,  76.,  77.,  78.,  79.],\n",
       "           [ 80.,  81.,  82.,  83.,  84.],\n",
       "           [ 85.,  86.,  87.,  88.,  89.],\n",
       "           [ 90.,  91.,  92.,  93.,  94.],\n",
       "           [ 95.,  96.,  97.,  98.,  99.]],\n",
       " \n",
       "          [[100., 101., 102., 103., 104.],\n",
       "           [105., 106., 107., 108., 109.],\n",
       "           [110., 111., 112., 113., 114.],\n",
       "           [115., 116., 117., 118., 119.],\n",
       "           [120., 121., 122., 123., 124.]],\n",
       " \n",
       "          [[125., 126., 127., 128., 129.],\n",
       "           [130., 131., 132., 133., 134.],\n",
       "           [135., 136., 137., 138., 139.],\n",
       "           [140., 141., 142., 143., 144.],\n",
       "           [145., 146., 147., 148., 149.]]]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 6\n",
    "img_size = 5\n",
    "groups = 3\n",
    "out_channels = 6\n",
    "vals = [x for x in range(1 * in_channels * img_size * img_size)] #torch.rand((1, in_channels, img_size, img_size))\n",
    "data = torch.tensor(vals, dtype=torch.float32).view((1, in_channels, img_size, img_size)) \n",
    "data.shape, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv1x1(in_channels, out_channels, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 5, 5])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_shuffle(x, groups):\n",
    "    # type: (torch.Tensor, int) -> torch.Tensor\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "    channels_per_group = num_channels // groups\n",
    "\n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups,\n",
    "               channels_per_group, height, width)\n",
    "\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "contiguous() -> Tensor\n",
       "\n",
       "Returns a contiguous tensor containing the same data as :attr:`self` tensor. If\n",
       ":attr:`self` tensor is contiguous, this function returns the :attr:`self`\n",
       "tensor.\n",
       "\u001b[0;31mType:\u001b[0m      method_descriptor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??torch.Tensor.contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = torch.rand((1000, 600, 50, 50)).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 ms, sys: 5.16 ms, total: 8.57 ms\n",
      "Wall time: 8.63 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 600, 50, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "shuffled_data = channel_shuffle(time_data, 10)\n",
    "shuffled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-41a76dc137f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrouped_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_res = data.view(1, groups, img_size, img_size)\n",
    "grouped_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed of Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 10000\n",
    "out_size = 2000\n",
    "bs = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = [conv1x1(in_size, out_size, g) for g in [1, 2, 5, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups: 1, Weights: torch.Size([2000, 10000, 1, 1])\n",
      "Groups: 2, Weights: torch.Size([2000, 5000, 1, 1])\n",
      "Groups: 5, Weights: torch.Size([2000, 2000, 1, 1])\n",
      "Groups: 10, Weights: torch.Size([2000, 1000, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for c in convs:\n",
    "    print(f\"Groups: {c.groups}, Weights: {c.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((out_size, in_size, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :: 6.357881048694253\n",
      "2 :: 3.564614233095199\n",
      "5 :: 0.9097015741281211\n",
      "10 :: 0.8136862958781421\n"
     ]
    }
   ],
   "source": [
    "for c in convs:\n",
    "    print(c.groups, \"::\", timeit.timeit(lambda: c(data), number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 2\n",
    "out_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = conv1x1(in_size, out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = conv1x1(in_size, out_size, groups=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 1, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[[ 0.1615]],\n",
       " \n",
       "          [[ 0.2314]]],\n",
       " \n",
       " \n",
       "         [[[-0.2153]],\n",
       " \n",
       "          [[-0.0568]]],\n",
       " \n",
       " \n",
       "         [[[-0.0012]],\n",
       " \n",
       "          [[ 0.2334]]],\n",
       " \n",
       " \n",
       "         [[[-0.1612]],\n",
       " \n",
       "          [[ 0.1873]]]], requires_grad=True))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.shape, a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 1, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.9219]]],\n",
       " \n",
       " \n",
       "         [[[-0.0821]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7273]]],\n",
       " \n",
       " \n",
       "         [[[-0.3001]]]], requires_grad=True))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight.shape, b.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_filter_vals_(c):\n",
    "    w_val = 1\n",
    "    groups = c.groups\n",
    "    for i in range(out_size):\n",
    "        for j in range(in_size // groups):\n",
    "            c.weight[i,j,0,0] = w_val\n",
    "            w_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_filter_vals_(a)\n",
    "easy_filter_vals_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1.]],\n",
       "\n",
       "         [[2.]]],\n",
       "\n",
       "\n",
       "        [[[3.]],\n",
       "\n",
       "         [[4.]]],\n",
       "\n",
       "\n",
       "        [[[5.]],\n",
       "\n",
       "         [[6.]]],\n",
       "\n",
       "\n",
       "        [[[7.]],\n",
       "\n",
       "         [[8.]]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]],\n",
       "\n",
       "\n",
       "        [[[3.]]],\n",
       "\n",
       "\n",
       "        [[[4.]]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = torch.ones((1,in_size,1,1)); data\n",
    "data = torch.tensor([1., 2.])\n",
    "data = data[None, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.]],\n",
       "\n",
       "         [[11.]],\n",
       "\n",
       "         [[17.]],\n",
       "\n",
       "         [[23.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.]],\n",
       "\n",
       "         [[2.]],\n",
       "\n",
       "         [[6.]],\n",
       "\n",
       "         [[8.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jkbc",
   "language": "python",
   "name": "jkbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
